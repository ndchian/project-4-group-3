{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporary code block for static csv\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"secondary_data.csv\", delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run sql engine and query data\n",
    "dataset = 'sql dataset file path goes here'\n",
    "engine = create_engine(f\"sqlite:///{dataset}\")\n",
    "conn = engine.connect()\n",
    "\n",
    "df = pd.read_sql(\"SELECT * FROM table\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df # view the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a summary table showing each column, its data type, number of unique values, and missing values\n",
    "summary_data = [(x, df[x].dtype,\n",
    "                 len(df[x].unique()),\n",
    "                 len(df[df[x].isna()])) for x in df.columns]\n",
    "\n",
    "summary_table = pd.DataFrame(summary_data, columns=['Column','Data Type',\n",
    "                                                    'Unique Values', 'Missing Values'])\n",
    "summary_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping columns with excessive missing values (30,000 or more)\n",
    "df = df.drop(columns=['stem-root', 'stem-surface',\n",
    "                      'veil-type', 'veil-color',\n",
    "                      'spore-print-color'], axis=1)\n",
    "\n",
    "# dropping remaining rows with na values\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating the target variable\n",
    "X = df.drop(columns='class')\n",
    "\n",
    "# separating categorical from numerical variables\n",
    "object_columns = [x for x in X if df[x].dtype == 'O']\n",
    "numeric_columns = [x for x in X if df[x].dtype != 'O']\n",
    "\n",
    "# getting dummies of the categorical variables\n",
    "dummies = pd.get_dummies(X[object_columns], dtype=int)\n",
    "\n",
    "# concatenating dummies to the numeric columns\n",
    "X = pd.concat([dummies, X[numeric_columns]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate PCA and fit the model\n",
    "pca = PCA(n_components=3)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "X_pca[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the PCA explained variance ratio\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting training and testing for regular dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df['class'])\n",
    "\n",
    "# splitting training and testing for pca dataset\n",
    "X_pca_train, X_pca_test, y_train, y_test = train_test_split(X_pca, y=['class'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
